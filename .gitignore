import requests
import asyncio
import aiohttp
import csv
import time
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor

# Target URL (Example: BBC News Technology section)
BASE_URL = "https://www.bbc.com/news/technology"
OUTPUT_FILE = "scraped_articles.csv"

# Function to get HTML content synchronously
def fetch_html(url):
    try:
        response = requests.get(url, timeout=5)
        response.raise_for_status()
        return response.text
    except requests.RequestException as e:
        print(f"Error fetching {url}: {e}")
        return None

# Function to parse articles from HTML
def parse_articles(html):
    soup = BeautifulSoup(html, 'html.parser')
    articles = []
    for item in soup.select(".gs-c-promo-heading"):
        title = item.get_text()
        link = item.get("href")
        if link and not link.startswith("http"):
            link = "https://www.bbc.com" + link
        articles.append((title, link))
    return articles

# Asynchronous function to fetch multiple pages asynchronously
async def fetch_async(session, url):
    try:
        async with session.get(url, timeout=5) as response:
            return await response.text()
    except Exception as e:
        print(f"Async error fetching {url}: {e}")
        return None

async def scrape_with_async(urls):
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_async(session, url) for url in urls]
        return await asyncio.gather(*tasks)

# Function to save results to CSV
def save_to_csv(data, filename):
    with open(filename, "w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["Title", "Link"])
        writer.writerows(data)

# Main function
def main():
    start_time = time.time()
    
    # Fetch HTML synchronously
    print("Fetching main page...")
    html = fetch_html(BASE_URL)
    if not html:
        return
    
    print("Parsing articles...")
    articles = parse_articles(html)
    
    # Extract article links to scrape more details
    article_links = [link for _, link in articles[:5]]  # Limit to 5 links for demo
    
    print("Fetching article details asynchronously...")
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    article_htmls = loop.run_until_complete(scrape_with_async(article_links))
    
    # Further parsing could be done on `article_htmls` here if needed
    
    print("Saving data to CSV...")
    save_to_csv(articles, OUTPUT_FILE)
    
    print(f"Scraping completed in {time.time() - start_time:.2f} seconds.")
    print(f"Data saved to {OUTPUT_FILE}")

if __name__ == "__main__":
    main()
